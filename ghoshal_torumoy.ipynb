{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Pandas helps you with managing Data (csv, excel, txt, etc.)\n",
    "import numpy as np # Numpy helps you with Numerical Calculation (matrix, vectors operation, etc.)\n",
    "import sklearn # This is a very popular package full of data mining algorithms.\n",
    "from time import sleep\n",
    "\n",
    "# This enables inline plotting in this notebook.\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt # A popular package for making visualization.\n",
    "\n",
    "from bs4 import BeautifulSoup as bs # It makes web scraping easy.\n",
    "import requests # It helps to retrieve contents of a URL\n",
    "import re\n",
    "\n",
    "\n",
    "url= \"r'https://www.reddit.com/r/politicalfactchecking/\"\n",
    "request = requests.get(r'https://www.reddit.com/r/politicalfactchecking/')\n",
    "r = request.text\n",
    "\n",
    "#r = open('Part1')\n",
    "soup = bs(r, \"lxml\")\n",
    "siteTable = soup.find('div', attrs={'id': 'siteTable'})\n",
    "threads = siteTable.findAll('div', attrs={'data-subreddit':'politicalfactchecking'})\n",
    "urlofAllThreads = []\n",
    "\n",
    "\n",
    "def verdictSimplifier(thS):\n",
    "    if not (thS.find('span', attrs={'class':'linkflairlabel'})):\n",
    "        tempVerdict = \"None\"\n",
    "    else:\n",
    "        tempVerdict = thS.find('span', attrs={'class':'linkflairlabel'}).contents[0]\n",
    "    return tempVerdict\n",
    "\n",
    "def threadDataExtractor(th, i):\n",
    "    thread_id = th['id']\n",
    "    time = th.time['datetime']\n",
    "    author = th['data-author']\n",
    "    vote_count = th.find('div', attrs={'class':'score unvoted'}).contents[0]\n",
    "    comments = re.sub(\"\\D\", \"\", (th.find('a', attrs={'data-event-action':'comments'}).contents[0]))\n",
    "    verdict = verdictSimplifier(th)\n",
    "    title = th.find('a', attrs={'class':'title may-blank '}).contents[0]\n",
    "    urlOfThread = 'https://www.reddit.com'+str((th.find('a', attrs = {'class': 'thumbnail self may-blank '})).get('href'))\n",
    "    urlofAllThreads.append(urlOfThread)\n",
    "    tData.loc[i+1] = [thread_id, time, author, vote_count, comments, verdict, title, urlOfThread]\n",
    "\n",
    "tData = pd.DataFrame(columns=['ID','Time', 'Author', 'vote_count', 'Number_of_Comments', 'Verdict', 'Title', 'URL'])\n",
    "for i,data in enumerate(threads):\n",
    "    temp = threads[i]\n",
    "    threadDataExtractor(temp, i)\n",
    "tData.to_csv('ghoshal_torumoy_part1.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cTextMerger(cThs):\n",
    "    cText = cThs.find('div', attrs = {'class':'md'})\n",
    "    cParagraphs = cText.findAll('p')\n",
    "    blockQuote = cText.findAll('blockquote')\n",
    "    cTable = cText.findAll('pre')\n",
    "    clearText = \"\"\n",
    "    for i,text in enumerate(cParagraphs):\n",
    "        clearText = clearText + str(text.contents[0]) + '\\n\\n'\n",
    "    for i, text in enumerate(blockQuote):\n",
    "        clearText = clearText + str(text.contents[0]) + '\\n\\n'\n",
    "    for i, text in enumerate(cTable):\n",
    "        clearText = clearText + str(text.contents[0]) + '\\n\\n'\n",
    "    return clearText\n",
    "\n",
    "def parentID(cThs):\n",
    "    if not cThs.find('div', attrs={'class':'child'}).contents:\n",
    "        pid = \"root\"\n",
    "    else:\n",
    "        pid = str(cThs.find('p').a['name'])\n",
    "    return(pid)\n",
    "\n",
    "def cID(cThs):\n",
    "    if not cThs['id']:\n",
    "        cid = \"None\"\n",
    "    else:\n",
    "        cid = cThs['id']\n",
    "    return(cid)\n",
    "\n",
    "def commentInfoExtractor(cTh, i, num):\n",
    "    thread_id = cID(cTh)\n",
    "    author = cTh['data-author']\n",
    "    points = re.sub(\"\\D\",\"\",(cTh.find('span', attrs={'class':'score unvoted'}).contents[0]))\n",
    "    time = cTh.time['datetime']\n",
    "    parent_ID = parentID(cTh)\n",
    "    text = cTextMerger(cTh)\n",
    "    permalink = (cTh.find('a', attrs={'data-event-action':'permalink'}))['href']\n",
    "    cData.loc[i+1+num] = [thread_id, author, points, time, parent_ID, text, permalink]\n",
    "\n",
    "cData = pd.DataFrame(columns=['Comment_ID', 'Author', 'Points', 'Time', 'Parent_id', 'Text', 'Permalink'])\n",
    "\n",
    "def cThreadGeneratorForComments(siteName):\n",
    "    cRequest = requests.get(siteName)\n",
    "    cR = cRequest.text\n",
    "    #cR = open(siteName)\n",
    "    cSoup = bs(cR, \"lxml\")\n",
    "    cSiteTable = cSoup.find('div', attrs={'class':'sitetable nestedlisting'})\n",
    "    cThreads = cSiteTable.findAll('div', attrs={'data-subreddit-fullname':'t5_2uvnw'})\n",
    "    return cThreads\n",
    "\n",
    "def cDataGenerator(cThreadTemp, num):\n",
    "    for i,data in enumerate(cThreadTemp):\n",
    "        temp = cThreadTemp[i]\n",
    "        commentInfoExtractor(temp, i, num)\n",
    "\n",
    "\n",
    "num = 0\n",
    "for i,temp in enumerate(urlofAllThreads):\n",
    "    sleep(5)\n",
    "    cThreadData = cThreadGeneratorForComments(urlofAllThreads[i])\n",
    "    cDataGenerator(cThreadData, num)\n",
    "    num = num + len(cThreadData)\n",
    "\n",
    "cData.to_csv('ghoshal_torumoy_part2.csv')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
